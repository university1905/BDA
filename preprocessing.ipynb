{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for Music Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from mutagen.mp3 import MP3\n",
    "from mutagen.easyid3 import EasyID3\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to extract MFCC features from the audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(file_path,  num_mfcc = 20, max_length=500):\n",
    "    try:\n",
    "        # load audio file\n",
    "        audio, sr = librosa.load(file_path, sr=None)\n",
    "        # Extract MFCC features\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=num_mfcc)\n",
    "        #pad or truncate MFCC features to max_length\n",
    "        if mfccs.shape[1] < max_length:\n",
    "            mfccs = np.pad(mfccs, ((0,0), (0, max_length - mfccs.shape[1])), mode = 'constant')\n",
    "        else:\n",
    "            mfccs = mfccs[:, :max_length]\n",
    "        #return MFCC features\n",
    "        return mfccs.T\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get files from the specified directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to iterate through all files in a folder and its sub-folders\n",
    "def iterate_files(root_folder):\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for filename in files:\n",
    "            yield os.path.join(root, filename)\n",
    "            \n",
    "root_folder = 'fma_large'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting mfcc features and storing it in mongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "client = MongoClient('mongodb://localhost:27017')  # Connection URI for MongoDB\n",
    "db = client['BDA_Project']  \n",
    "collection = db['songs_data'] \n",
    "\n",
    "# iterate through audio files in the folder and its sub folders\n",
    "for file_path in tqdm(iterate_files(root_folder)):\n",
    "    # initializing lists to store flattened features and labels\n",
    "    # if file_path.endswith('000536.mp3'):\n",
    "    #     break\n",
    "    \n",
    "    if file_path.endswith('.mp3'):\n",
    "        # Extract features\n",
    "        mfcc = extract_mfcc(file_path)\n",
    "        if mfcc is not None:\n",
    "            #flatten the mfcc features\n",
    "            flat_mfcc = mfcc.flatten()\n",
    "            data = {\n",
    "                'feature': flat_mfcc.tolist(),\n",
    "                'path': file_path\n",
    "            }\n",
    "            collection.insert_one(data)\n",
    "            del flat_mfcc\n",
    "            del mfcc\n",
    "            del data\n",
    "            \n",
    "client.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the meta data of the songs and storing it in MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "316it [00:00, 678.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# Connect to MongoDB\n",
    "client = MongoClient('mongodb://localhost:27017')  # Connection URI for MongoDB\n",
    "db = client['BDA_Project']  \n",
    "collection = db['songs_meta_data'] \n",
    "\n",
    "# iterate through audio files in the folder and its sub folders\n",
    "for file_path in tqdm(iterate_files(root_folder)):\n",
    "    if file_path.endswith('.mp3'):\n",
    "        # Load the MP3 file\n",
    "        audio = MP3(file_path, ID3=EasyID3)\n",
    "        # Extract metadata\n",
    "        metadata = {}\n",
    "        metadata[\"path\"] = file_path\n",
    "        for key in audio.keys():\n",
    "            if key in [\"album\", \"title\", \"artist\", \"genre\", \"date\"]: \n",
    "                metadata[key] = audio[key][0]\n",
    "        \n",
    "        collection.insert_one(metadata)\n",
    "        del metadata\n",
    "        \n",
    "    # if file_path.endswith('000536.mp3'):\n",
    "    #     break\n",
    "            \n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<prompt>\n",
    "I need to make a readme file for the following project. The files run in the following order:\n",
    "1. preprocess.ipynb (preprocesses the data and stores it in MongoDB)\n",
    "2. music_recommendation_system_using_Kmeans.py (performs Kmeans clustering on the data)\n",
    "3. Kmeans_evaluation.py (makes sillouhette score and elbow plot to evaluate the Kmeans clustering)\n",
    "4. app.py (Flask app that serves the recommendation system and also calls the producer and consumer functions as needed.)\n",
    "<prompt/>\n",
    "<preprocess.ipynb>\n",
    "    File attached above\n",
    "<preprocess.ipynb/>\n",
    "\n",
    "<music_recommendation_system_using_Kmeans.py>\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pymongo\n",
    "    import pandas as pd\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType\n",
    "    from pyspark.sql.functions import substring\n",
    "    from pyspark.sql.functions import udf\n",
    "    from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "    from pyspark.ml.clustering import KMeans\n",
    "\n",
    "    # Initialize Spark\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Music Recommendation Model\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Connect to MongoDB\n",
    "    client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client[\"BDA_Project\"]\n",
    "    collection = db[\"songs_data\"]\n",
    "\n",
    "    # Retrieve data from the collection\n",
    "    cursor = collection.find()\n",
    "\n",
    "    # Convert the data to a Pandas DataFrame\n",
    "    df_pandas = pd.DataFrame(list(cursor))\n",
    "    # Drop the _id field from the DataFrame\n",
    "    df_pandas = df_pandas.drop(columns=['_id'])\n",
    "    # Convert the array elements to floating-point values\n",
    "    df_pandas[\"feature\"] = df_pandas[\"feature\"].apply(lambda x: [float(i) for i in x])\n",
    "\n",
    "    # Define the schema for the Spark DataFrame\n",
    "    schema = StructType([\n",
    "        StructField(\"feature\", ArrayType(FloatType()), True),\n",
    "        StructField(\"path\", StringType(), True), \n",
    "    ])\n",
    "\n",
    "    df = spark.createDataFrame(df_pandas, schema = schema)\n",
    "\n",
    "    array_to_vector_udf = udf(lambda arr: Vectors.dense(arr), VectorUDT())\n",
    "    df = df.select(array_to_vector_udf(df['feature']).alias('features'), df['path'])\n",
    "\n",
    "    # Extract the number from the \"path\" column\n",
    "    df = df.withColumn(\"label\", substring(\"path\", 15, 6))\n",
    "\n",
    "    # Convert the \"number\" column to integer type\n",
    "    df = df.withColumn(\"label\", df[\"label\"].cast(\"int\"))\n",
    "\n",
    "    kmeans = KMeans(featuresCol = \"features\", k = 10, seed = 1)\n",
    "    model = kmeans.fit(df)\n",
    "\n",
    "    predictions = model.transform(df)\n",
    "\n",
    "    feature_rdd = df.select(\"label\", \"features\", \"path\").rdd.map(lambda x: (x[0], x[1], x[2]))\n",
    "    model.save('kmeans_model')\n",
    "\n",
    "    # Save feature_rdd locally\n",
    "    feature_rdd.saveAsPickleFile(\"feature_rdd.pickle\")\n",
    "    df.write.parquet('df.parquet')\n",
    "\n",
    "    # Stop Spark session\n",
    "    spark.stop()\n",
    "<music_recommendation_system_using_Kmeans.py/>\n",
    "\n",
    "<Kmeans_evaluation.py>\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.ml.clustering import KMeansModel\n",
    "    from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "    # Initialize Spark\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"kmeans evaluation for Similar Songs\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # # Load saved feature_rdd\n",
    "    df = spark.read.parquet('df.parquet')\n",
    "\n",
    "    model = KMeansModel.load('kmeans_model')\n",
    "    predictions = model.transform(df)\n",
    "\n",
    "    # clusterin evaluations\n",
    "    evaluator_silhouette = ClusteringEvaluator(featuresCol= 'features', predictionCol= 'prediction', metricName= 'silhouette')\n",
    "    silhouette_score = evaluator_silhouette.evaluate(predictions)\n",
    "\n",
    "    print(\"Silhouette Score:\", silhouette_score)\n",
    "\n",
    "    spark.stop()\n",
    "<Kmeans_evaluation.py/>\n",
    "\n",
    "<app.py>\n",
    "    from flask import Flask, render_template, send_from_directory, url_for\n",
    "    from pymongo import MongoClient\n",
    "    from bson.objectid import ObjectId\n",
    "    import os\n",
    "    import requests\n",
    "    from producer import send_song_data\n",
    "    import json\n",
    "    from flask import jsonify\n",
    "    import time\n",
    "\n",
    "\n",
    "    client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client.BDA_Project\n",
    "    collection = db.songs_meta_data\n",
    "\n",
    "    app = Flask(__name__)\n",
    "\n",
    "    LASTFM_API_KEY = '31fe3d279288de0a7f8997ffb8cba2ab'\n",
    "\n",
    "    def get_album_art(artist, album):\n",
    "        url = \"http://ws.audioscrobbler.com/2.0/\"\n",
    "        params = {\n",
    "            \"method\": \"album.getinfo\",\n",
    "            \"api_key\": LASTFM_API_KEY,\n",
    "            \"artist\": artist,\n",
    "            \"album\": album,\n",
    "            \"format\": \"json\"\n",
    "        }\n",
    "        response = requests.get(url, params=params)\n",
    "        data = response.json()\n",
    "        if response.status_code == 200 and 'album' in data:\n",
    "            return data['album'].get('image', [{}])[-1].get('#text')  # Get the largest image\n",
    "        else:\n",
    "            return url_for('static', filename='default.jpg')\n",
    "\n",
    "    def load_audio_metadata():\n",
    "        audio_files = []\n",
    "        for item in collection.find():\n",
    "            audio_files.append(item)\n",
    "        return audio_files\n",
    "\n",
    "    audio_files = load_audio_metadata()\n",
    "\n",
    "    @app.route('/audio/<path:filename>')\n",
    "    def send_audio(filename):\n",
    "        # Convert backslashes to forward slashes\n",
    "        filename = filename.replace('\\\\', '/')\n",
    "        # directory = r'D:\\University\\Semester 4\\BIG DATA\\project'\n",
    "        directory = r'/home/hdoop/Documents/project/'\n",
    "        full_path = os.path.join(directory, filename)\n",
    "        if not os.path.exists(full_path):\n",
    "            print(f\"File not found: {full_path}\")  # Debug output\n",
    "            return \"File not found\", 404\n",
    "        return send_from_directory(directory, filename, as_attachment=False)\n",
    "\n",
    "\n",
    "    @app.route('/')\n",
    "    def home():\n",
    "        return render_template('index.html', audio_files=audio_files)\n",
    "\n",
    "\n",
    "    @app.route('/song/<song_id>')\n",
    "    def song_details(song_id):\n",
    "        \n",
    "        song = collection.find_one({'_id': ObjectId(song_id)})\n",
    "        album_art_url = get_album_art(song['artist'], song['album'])\n",
    "        song_path = song['path']\n",
    "        song_number = int(song_path[15:20:1])\n",
    "        send_song_data(song_number)\n",
    "\n",
    "        start_time = time.time()\n",
    "        timeout = 20\n",
    "        while time.time() - start_time < timeout:\n",
    "            try:\n",
    "                with open('recommended_songs.json', 'r') as json_file:\n",
    "                    similar_songs = json.load(json_file)\n",
    "                if similar_songs.get('song_id') == song_number:\n",
    "                    break\n",
    "            except (IOError, ValueError, KeyError):\n",
    "                pass\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            return 'Recommendation processing timeout.', 503\n",
    "        \n",
    "        similar_songs_path = similar_songs.get('path')\n",
    "\n",
    "        print(similar_songs_path)\n",
    "        similar_songs_cursor = collection.find({\"path\": {\"$in\": similar_songs_path}})\n",
    "        similar_songs_list = list(similar_songs_cursor)\n",
    "        similar_songs_dict = {song['path']: song for song in similar_songs_list}\n",
    "        ordered_similar_songs = [similar_songs_dict[path] for path in similar_songs_path if path in similar_songs_dict]\n",
    "\n",
    "        if song:\n",
    "            return render_template('song.html', song=song, album_art_url=album_art_url, similar_songs = ordered_similar_songs)\n",
    "        else:\n",
    "            return 'Song not found', 404\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        app.run(debug=True)\n",
    "\n",
    "<app.py/>\n",
    "\n",
    "<producer.py>\n",
    "    from kafka import KafkaProducer\n",
    "    import json\n",
    "\n",
    "    # Initialize Kafka Producer\n",
    "    producer = KafkaProducer(bootstrap_servers=['localhost:9092'],\n",
    "                            value_serializer=lambda x: json.dumps(x).encode('utf-8'))\n",
    "\n",
    "\n",
    "    def send_song_data(song_id):\n",
    "        message = {'song_number': song_id}\n",
    "        producer.send('topic', value=message)\n",
    "        print(f\"Sent song to topic: {message}\")\n",
    "        producer.flush()\n",
    "<producer.py/>\n",
    "\n",
    "<consumer.py>\n",
    "    from kafka import KafkaConsumer\n",
    "    import json\n",
    "    from pyspark.ml.clustering import KMeansModel\n",
    "    from scipy.spatial.distance import cosine\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    # Initialize Spark\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Query Annoy Index for Similar Songs\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Load saved feature_rdd\n",
    "    feature_rdd = spark.sparkContext.pickleFile(\"feature_rdd.pickle\")\n",
    "    df = spark.read.parquet('df.parquet')\n",
    "\n",
    "    model = KMeansModel.load('kmeans_model')\n",
    "    predictions = model.transform(df)\n",
    "\n",
    "    # Initialize Kafka Consumer\n",
    "    consumer = KafkaConsumer(\n",
    "        'topic',\n",
    "        bootstrap_servers=['localhost:9092'],\n",
    "        # auto_offset_reset='earliest',\n",
    "        value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
    "    )\n",
    "\n",
    "    def get_similar_songs_path(song_number, similar_song_numbers):\n",
    "        similar_songs_paths = []\n",
    "        for number in similar_song_numbers:\n",
    "            if song_number ==number:\n",
    "                continue\n",
    "            # Filter the RDD to find the record(s) with the matching number\n",
    "            matching_record = feature_rdd.filter(lambda x: x[0] == number)\n",
    "            matching_record = matching_record.first()\n",
    "            corresponding_path = matching_record[2]\n",
    "            similar_songs_paths.append(corresponding_path)\n",
    "        \n",
    "        return similar_songs_paths\n",
    "\n",
    "    def recommend_top_songs(song_number, predictions, input_song_features,top_n = 10):\n",
    "        # Find the cluster assigned to the given song\n",
    "        cluster = predictions.filter(predictions['Label'] == song_number).select('prediction').collect()[0]['prediction']\n",
    "        \n",
    "        # Get other songs belonging to the same cluster\n",
    "        recommended_songs = predictions.filter(predictions['prediction'] == cluster).select('Label', 'Features').collect()\n",
    "        \n",
    "        # # Filter out the input song from the recommendations\n",
    "        recommended_songs = [(song['Label'], song['Features']) for song in recommended_songs if song['Label'] != song_number]\n",
    "        \n",
    "        # Calculate similarity scores between the input song and recommended songs\n",
    "        similarity_scores = []\n",
    "        for recommended_song_id, recommended_song_features in recommended_songs:\n",
    "            similarity_score = cosine(input_song_features, recommended_song_features)\n",
    "            similarity_scores.append((recommended_song_id, similarity_score))\n",
    "        \n",
    "        # Sort recommended songs based on similarity scores\n",
    "        ranked_recommendations = sorted(similarity_scores, key=lambda x: x[1])\n",
    "        \n",
    "        # Return the top N recommended songs\n",
    "        top_recommendations = [song_id for song_id, _ in ranked_recommendations[:top_n]]\n",
    "\n",
    "        return top_recommendations\n",
    "\n",
    "\n",
    "    for message in consumer:\n",
    "        print(\"Consumer started running\")\n",
    "        message_data = message.value\n",
    "        song_number = message_data['song_number']\n",
    "        print(f\"Received song for processing: {song_number}\")\n",
    "\n",
    "        song_features = feature_rdd.lookup(song_number)[0]\n",
    "        similar_song_numbers = recommend_top_songs(song_number, predictions, feature_rdd.lookup(song_number)[0])\n",
    "        similar_songs_path = get_similar_songs_path(song_number, similar_song_numbers)\n",
    "\n",
    "        similar_songs = {'song_id': song_number, 'path': similar_songs_path}\n",
    "        print(similar_songs)\n",
    "        with open('recommended_songs.json', 'w') as json_file:\n",
    "            json.dump(similar_songs, json_file)\n",
    "\n",
    "<consumer.py/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Recommendation System\n",
    "\n",
    "The project assigned focused on developing a streamlined alternative to Spotify by creating a sophisticated music recommendation and streaming system. This system is designed to assess and implement cutting-edge machine learning algorithms and big data technologies for effective music information retrieval (MIR). The project is structured into several phases: creating an ETL pipeline to process and store features from a large music dataset, utilizing Apache Spark for training a music recommendation model, deploying the model in a Flask-based web application, and using Apache Kafka for real-time music recommendation generation.\n",
    "\n",
    "## Files\n",
    "\n",
    "### 1. `preprocess.ipynb`\n",
    "\n",
    "This piece of code is responsible for the initial preprocessing of the data. It includes loading the Free Music Archive (FMA) dataset, extracting essential audio features using techniques such as Mel-Frequency Cepstral Coefficients (MFCC), and storing these features in a MongoDB database for easy access and manipulation later in the project.\n",
    "\n",
    "### 2. `music_recommendation_system_using_Kmeans.py`\n",
    "\n",
    "This Python script uses Apache Spark to perform K-means clustering on the preprocessed data stored in MongoDB. The main goal is to group similar songs based on their features to enhance the recommendation process.\n",
    "\n",
    "### 3. `Kmeans_evaluation.py`\n",
    "\n",
    "This script evaluates the K-means clustering model implemented in the previous file. It generates evaluation metrics like the silhouette score and elbow plot, which help in assessing the clustering performance and determining the optimal number of clusters.\n",
    "\n",
    "### 4. `app.py`\n",
    "\n",
    "This Flask application serves as the frontend of the music recommendation system. It interacts with MongoDB to fetch song data, displays song details, and manages user interactions. The app also integrates Kafka to handle real-time music recommendations based on user activity.\n",
    "\n",
    "### Additional Files\n",
    "\n",
    "#### `producer.py`\n",
    "\n",
    "This script acts as a Kafka producer. It sends song data to a specified Kafka topic whenever a song is played, enabling real-time processing and recommendation.\n",
    "\n",
    "#### `consumer.py`\n",
    "\n",
    "Complementing the producer, this Kafka consumer listens for messages (song plays) and triggers the recommendation process based on the current song and user activity. It uses the features and models stored to recommend similar songs.\n",
    "\n",
    "---\n",
    "\n",
    "## Installation and Usage\n",
    "\n",
    "### Setting Up the Environment\n",
    "1. Ensure Kafka and MongoDB services are running.\n",
    "2. Create a topic in Kafka named `topic`.\n",
    "\n",
    "### Execution Instructions\n",
    "1. **Start the Kafka Producer**:  \n",
    "   Run `python producer.py` to begin data ingestion into Kafka.\n",
    "2. **Execute the Kafka Consumer**:  \n",
    "   Run `python apriori.py` to start the consumer process that applies the Apriori algorithm and writes the output to MongoDB.\n",
    "   Run `python pcy.py` to start the consumer process that applies the pcy algorithm and writes the output to MongoDB.\n",
    "   Run `python custom.py` to start the consumer process that applies the custom algorithm and writes the output to MongoDB.\n",
    "3. **Activate MongoDB & Open Mongosh**:      Activate Mongo by typing `sudo systemctl start mongod` terminal\n",
    "   Run Mongosh Terminal by typing `mongosh` in the terminal\n",
    "   Run `use mydatabase` to select the database  4. **View the Database for the PCY consumer**:  \n",
    "   Run `db.frequent_itemsets.deleteMany({})` to delete the previous content for the PCY consumer database\n",
    "   Run `db.frequent_itemsets.find().pretty()` to view the database for the PCY consumer 5. **View the Database for the Apriori consumer**:      Run `db.apriori.deleteMany({})` to delete the previous content for the Apriori consumer database\n",
    "   Run `db.apriori.find().pretty()` to view the database for the Apriori consumer\n",
    "6. **View the Database for the custom consumer**:      Run `db.custom.deleteMany({})` to delete the previous content for the custom consumer database\n",
    "   Run `db.custom.find().pretty()` to view the database for the custom consumer\n",
    "\n",
    "\n",
    "## Contributors\n",
    "- **Hamza Burney** || 22i-2058\n",
    "- **Irtiza Abbas** || 22I-1862\n",
    "- **Zain Abbas** || 22I-1905\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
